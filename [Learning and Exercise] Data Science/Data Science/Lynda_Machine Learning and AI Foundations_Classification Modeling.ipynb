{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning and AI Foundation: Classfication Modeling and frequent questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice Architecture of Phases and tasks:\n",
    "    https://s2.smu.edu/~mhd/8331f03/crisp.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Concept:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Interaction:\n",
    "   - Let's say we have variables that are highly correlated or \"explain the same thing.\"  \n",
    "   - If you have high collinearity among e.g,. 2 variables, the decision tree would automatically pick the \"better\" one for the split. (tree is good for uncovering complex dependencies among predictor variables.)  \n",
    "        -e.g.: Car price:  \n",
    "        __price = -55089.98 + 87.34 x engine size + 60.93 x horse power + 770.42 width__  \n",
    "        (never takes into consideration that engine size may be related to the width of the car.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - __Interaction term__: captures the interaction between engine and width (__main effect__)  \n",
    "        * price = β0 + β1 x engine size + β2 x horse power + β3 x width + β4 x(engine size x width)\n",
    "        * price = β0 + (β1 + β4. width) engine size + β2. horse power + β3. width  \n",
    "    (Now, β4 can be interpreted as the impact on the engine size if the width is increased by 1 unit.) \n",
    "  - __Interaction in CARTree__:\n",
    "    - https://www.youtube.com/watch?v=NgwJWWlDdwk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image/tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gradient boosting & XGBoost:\n",
    "https://medium.com/@thems18/gentle-introduction-of-xgboost-library-2b1ac2669680  \n",
    "\n",
    "- __Gradient boosting__ is a supervised learning technique for __regression and classification__ problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n",
    "- It’s called gradient boosting because it uses a gradient descent algorithm to __minimize the loss when adding new models__.  \n",
    "- __Specify which booster to use__: gbtree, gblinear or dart.\n",
    "- The gradient boosting algorithm improves on Fm by constructing a new model that adds an estimator h to provide a better model:\n",
    "    <img src=\"Image/fm.svg\">\n",
    "    - To find h, the gradient boosting solution starts with the observation that a perfect h would imply\n",
    "    <img src=\"Image/fm2.svg\">\n",
    "    or, equivalently:\n",
    "    <img src=\"Image/fm3.svg\">\n",
    "- __XGBoost__ is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n",
    "- XGBoost provides a __parallel tree boosting__ (also known as GBDT, GBM), and the same code runs on the major distributed environment (Hadoop, SGE, MPI), because __core algorithm is parallelizable__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SVM-Support Vector Machine:\n",
    "https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72\n",
    "https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-coding-edd8f1cf8f2d\n",
    "\n",
    "- SVM:  \n",
    "    __input__: given __labeled training data__(supervised learning), __outputs:__ an optimal hyperplane which categorizes new examples. (Means: Separation of classes: find out a line/ hyper-plane (in multidimensional space that separate outs classes).)\n",
    "<img src=\"Image/svm.png\">\n",
    "- __Parameters:__\n",
    "    Varying those we can achive considerable non linear classification line with more accuracy in reasonable amount of time. \n",
    "    - __1. Kernel:__   \n",
    "    the equation for prediction for a new input using the dot product between the input (x) and each support vector (xi) is calculated as follows:  \n",
    "    __f(x) = B(0) + sum(ai * (x,xi))__  \n",
    "    This is an equation that involves calculating the inner products of a new input vector (x) with all support vectors in training data.\n",
    "    - __2. Regularization:__ (C parameter in python’s sklearn library) \n",
    "    Tells the SVM optimization how much you want to avoid misclassifying each training example.\n",
    "        - a. __Large values of C__: the optimization will choose a __smaller-margin__ hyperplane if that hyperplane does a better job of getting all the training points classified correctly.\n",
    "        - b. __very small value of C__: will cause the optimizer to look for a __larger-margin__ separating hyperplane, even if that hyperplane misclassifies more points.\n",
    "        <img src=\"Image/reg.png\">\n",
    "    - __3. Gamma:__  \n",
    "    Defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’:\n",
    "        - __Low gamma:__ points far away from plausible seperation line are considered in calculation for the seperation line.\n",
    "        - __High gamma:__ the points close to plausible line are considered in calculation.\n",
    "        <img src=\"Image/gamma.png\">\n",
    "    - __4. Margin:__  \n",
    "        - A separation of line to the closest class points.\n",
    "        - Very importrant characteristic of SVM classifier. SVM to core tries to achieve a good margin.\n",
    "        - __Good margin:__ one where this separation is larger for both the classes. (allows the points to be in their respective classes without crossing to other class.)\n",
    "        <img src=\"Image/margin2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bit about Bayes:\n",
    "#### 1. Bayes Theorem:\n",
    "  <img src=\"Image/b.png\">\n",
    "- e.g.: the probability that price of a house is high, can be better assessed if we know the facilities around it, compared to the assessment made without the knowledge of location of the house.  \n",
    "#### 2. What it for?\n",
    "- its possible to build a learner that predicts the probability of the response variable belonging to some class, given a new set of attributes.  \n",
    "\n",
    "#### 3. Naive Bayes Algorithm:  \n",
    "- __What?__ \n",
    "    - Not a single algorithm but a family of algorithms;\n",
    "    - Making an assumption of conditional independence over the training dataset. This drastically reduces the complexity of above mentioned problem to just 2n. (This assumption makes the Bayes algorithm, naive.)  \n",
    "- __Where?__\n",
    "    - __Text classification/ Spam Filtering/ Sentiment Analysis:__    \n",
    "        - Due to better result in multi class problems and independence rule  \n",
    "        - Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)  \n",
    "        - Recommendation System: filter unseen information and predict whether a user would like a given resource or not.  \n",
    "    - __Real time Prediction:__       \n",
    "        - Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.  \n",
    "    - __Multi class Prediction:__   \n",
    "        - predict the probability of multiple classes of target variable.  \n",
    "        \n",
    "- __How?__    \n",
    "    Given, n different attribute values, the likelihood now can be written as:  \n",
    "(X： the attributes or features  \n",
    " Y： variable.  \n",
    " P(X|Y) becomes equal to the products of, probability distribution of each attribute X given Y.)  \n",
    "<img src=\"Image/b2.png\">\n",
    "- __Why?__    \n",
    "    - Work really well in __complex situations__, despite the simplified assumptions and naivety.  \n",
    "    - Require __small number of training data__ for estimating the parameters necessary for classification.<br>\n",
    "\n",
    "- __Type:__ \n",
    "    - __Gaussian__: \n",
    "        - It is used in classification  \n",
    "        - Assumes that features follow a <font color='red'>__normal distribution.__</font>\n",
    "    - __Bernoulli__:  \n",
    "        - The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). \n",
    "        - One application would be text classification with ‘bag of words’ model where the __1s & 0s are “word occurs in the document”__ and __“word does not occur in the document” respectively__.  \n",
    "    - __Multinomial__:   \n",
    "        - It is used for discrete counts.  \n",
    "        - e.g.: text classification problem: Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have __“count how often word occurs in the document”__, or “number of times outcome number x_i is observed over the n trials”.  \n",
    "\n",
    "- __Examples:__\n",
    "    - A fruit may be considered to be an apple if it is red, round, and about 3″ in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features.  \n",
    "    - Predict the class of another fruit based on training set:\n",
    "        - From 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow\n",
    "        - Out of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow\n",
    "        - From the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow\n",
    "        <img src = 'Image/fruit.png'>\n",
    "        - Conclusion: In this case, based on the higher score (0.01875 lt 0.252) we can assume this Long, Sweet and Yellow fruit is, in fact, a __Banana__.  \n",
    "     \n",
    "- __Disadvantage:__\n",
    "    - Main disadvantage is that it <font color='red'> can’t learn interactions between features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bayes Network:\n",
    "- A type of probabilistic graphical model that uses Bayesian inference for __probability computations__. (directed acyclic graph)\n",
    "- __For what?__\n",
    "    - Aim to model __conditional dependence__, and therefore __causation__, by representing conditional dependence by edges in a directed graph. \n",
    "    <img src='Image/b3.jpeg'>\n",
    "- __Meaning:__ \n",
    "    - __edge (A, B)__: it means that P(B|A) is a factor in the joint probability distribution, so we must know P(B|A) for all values of B and A in order to conduct inference. \n",
    "    - e.g.: P(WetGrass|Rain) will be a factor, whose probability values are specified next to the WetGrass node in a conditional probability table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
