{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning and AI Foundation: Classfication Modeling and frequent questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice Architecture of Phases and tasks:\n",
    "    https://s2.smu.edu/~mhd/8331f03/crisp.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definitions about Measurements:\n",
    "    - Binary\n",
    "    - Ordinal;\n",
    "    - Scale;\n",
    "    - Nominal;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why statistical Classifiers?\n",
    "   - Transparent;\n",
    "   - Scalable;\n",
    "   - Could be included in ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <font color='red'>Common Problems</font>\n",
    "   - Missing Data;\n",
    "   - Feature selection;\n",
    "   - Interactions;\n",
    "   - Overfitting;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Issues to Attend To:\n",
    "   - Does it use all or some of the inputs?\n",
    "   - Does it use all or some of the cases?\n",
    "   - Situations where it might perform well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Steps for data mining:\n",
    "https://www.the-modeling-agency.com/crisp-dm.pdf\n",
    "    <img src='Image/phases.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discriminant Analysis:\n",
    "   - Typically, all inputs are used;\n",
    "   - Typically, listwise deletion of missing data;\n",
    "   - Some iplementations will impute;\n",
    "   - Stepwise variable selection;\n",
    "   - Scale variables only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Missing Data:\n",
    "   - Decision Tree is the exception; (Be careful when trees consistantly 'win')\n",
    "   - __Listwise deletion__ is the most common;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Imputation:\n",
    "   - Can be as simple as replacing with __mean__?\n",
    "   - Can be as complex as a __neural network__?\n",
    "   - Too important to leave to chance\n",
    "   - Not the only solution\n",
    "- Note:  <font color='red'>if you have more data missed, have to consider other possibilities</font>:\n",
    "        - Build 1 model on complete data: like NN;\n",
    "        - Build another model on incomplete data: like Decision Tree. (because DT could not be considered imputation, but other models do.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Bias-Variance Trade-off:\n",
    "   - High Bias:\n",
    "        - Under fit\n",
    "        - TODO1: Add interaction terms;  \n",
    "        - TODO2: Add inputs;  \n",
    "        - TODO3: Grow more aggressively;  \n",
    "   - High Variance:\n",
    "        - Model is sensitive to noise;\n",
    "        - Generalized poorly;\n",
    "        - Overfit; (Complicated model will not work well!)  \n",
    "__Note:__ <font color='red'>Sometimes need to try more complex model and less complex model;</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Data Reduction: (Feature selection)\n",
    "   - __What__:\n",
    "       - Remove the poor and redundant predctors before modeling;  \n",
    "   - __Why__:  \n",
    "       - No algorithm is immunue to modeling noise.  \n",
    "       (The noise in training & test set are not the same.)\n",
    "   - __Don\\'t the algorithms do this? Because...__\n",
    "        - Not all data: Trees & stepwise (don't use all the variables);\n",
    "        - By weights: Like neural nets (can assign low weights);\n",
    "        - By extra features: Some implementations;\n",
    "   - __TODO:__\n",
    "       - 1. Try to run all of the 'Bi-variates'(1 variable and the target);\n",
    "       - 2. Try to identify redundancy; (e.g.: Weight, height...)\n",
    "       - 3. Turn on/off some features: Some implementations have dedicated features;\n",
    "       - 4. Boosting can offer a powerful trick;  \n",
    "       __Note:__ Not one is the best! Modeling challenges are best addressed empirically.  \n",
    "       <font color='orange'>??? Redundancy VS. Interaction</font>  \n",
    "       <font color='orange'>??? How to detect interaction? Based on pair plot?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (s curve):  \n",
    "- Arguably the most <font color='red'>Transparent</font>- Lots of details\n",
    "<img src='Image/LG.png'>\n",
    "- All inputs are either used or stepwise;  \n",
    "- Listwise deletion; \n",
    "- Stepwise variable selection;  \n",
    "- All inputs are clearly __ranked__;  \n",
    "- Each input has a test of significance;  \n",
    "- Each input has a __measured effect__;  \n",
    "- Scale variables only;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree:\n",
    "- Many different types;\n",
    "- Variety of __missing data-handling__ options;\n",
    "- Not all inputs are used-Greedy;\n",
    "- All types of variables;  \n",
    "__Note:__ If DT works well in some case, we have to check how many records have been ran on the other model, and how the performance is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors:\n",
    "- 'Lazy' learner;\n",
    "- Nearest: Euclidean distance;\n",
    "- All inputs are used;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppport Vector Machine:\n",
    "- Black Box...\n",
    "- All inputs are used;\n",
    "- Robust for a large number of inputs;\n",
    "- Important parameter-C: influences the __bias-variance trade-off__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network:\n",
    "- Black Box...\n",
    "- Deep learning is a kind of neural network\n",
    "- All inputs are used: input screening is helpful.\n",
    "- Key feature-backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks:\n",
    "- Bayes\\' theorem: Combining probabilities;\n",
    "- 3 ways to address interactions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balance:\n",
    "<img src='Image/balance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions:\n",
    "- If the accuracy is bad, __interaction__ should be the 1st thing to check.\n",
    "- Curvilinearity is a special kind of interaction.\n",
    "- Some tech address interactions but in __hidden ways__.\n",
    "- <font color='red'>Trees</font> are 1 of the easiest ways to find interactions.\n",
    "- Methods create own interaction term:\n",
    "    - Trees;\n",
    "    - Logistic;\n",
    "    - Discriminant;\n",
    "- E.g.:\n",
    "    'Salary' and 'Edu level' should be the same, but these 2 are different, because of the 3rd variables: __Sex__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Concept:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Interaction:\n",
    "   - Let's say we have variables that are highly correlated or \"explain the same thing.\"  \n",
    "   - If you have high collinearity among e.g,. 2 variables, the decision tree would automatically pick the \"better\" one for the split. (tree is good for uncovering complex dependencies among predictor variables.)  \n",
    "        -e.g.: Car price:  \n",
    "        __price = -55089.98 + 87.34 x engine size + 60.93 x horse power + 770.42 width__  \n",
    "        (never takes into consideration that engine size may be related to the width of the car.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - __Interaction term__: captures the interaction between engine and width (__main effect__)  \n",
    "        * price = β0 + β1 x engine size + β2 x horse power + β3 x width + β4 x(engine size x width)\n",
    "        * price = β0 + (β1 + β4. width) engine size + β2. horse power + β3. width  \n",
    "    (Now, β4 can be interpreted as the impact on the engine size if the width is increased by 1 unit.) \n",
    "  - __Interaction in CARTree__:\n",
    "    - https://www.youtube.com/watch?v=NgwJWWlDdwk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image/tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gradient boosting & XGBoost:\n",
    "https://medium.com/@thems18/gentle-introduction-of-xgboost-library-2b1ac2669680  \n",
    "\n",
    "- __Gradient boosting__ is a supervised learning technique for __regression and classification__ problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n",
    "- It’s called gradient boosting because it uses a gradient descent algorithm to __minimize the loss when adding new models__.  \n",
    "- __Specify which booster to use__: gbtree, gblinear or dart.\n",
    "- The gradient boosting algorithm improves on Fm by constructing a new model that adds an estimator h to provide a better model:\n",
    "    <img src=\"Image/fm.svg\">\n",
    "    - To find h, the gradient boosting solution starts with the observation that a perfect h would imply\n",
    "    <img src=\"Image/fm2.svg\">\n",
    "    or, equivalently:\n",
    "    <img src=\"Image/fm3.svg\">\n",
    "- __XGBoost__ is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n",
    "- XGBoost provides a __parallel tree boosting__ (also known as GBDT, GBM), and the same code runs on the major distributed environment (Hadoop, SGE, MPI), because __core algorithm is parallelizable__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SVM-Support Vector Machine:\n",
    "https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72\n",
    "https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-coding-edd8f1cf8f2d\n",
    "\n",
    "- SVM:  \n",
    "    __input__: given __labeled training data__(supervised learning), __outputs:__ an optimal hyperplane which categorizes new examples. (Means: Separation of classes: find out a line/ hyper-plane (in multidimensional space that separate outs classes).)\n",
    "<img src=\"Image/svm.png\">\n",
    "- __Parameters:__\n",
    "    Varying those we can achive considerable non linear classification line with more accuracy in reasonable amount of time. \n",
    "    - __1. Kernel:__   \n",
    "    the equation for prediction for a new input using the dot product between the input (x) and each support vector (xi) is calculated as follows:  \n",
    "    __f(x) = B(0) + sum(ai * (x,xi))__  \n",
    "    This is an equation that involves calculating the inner products of a new input vector (x) with all support vectors in training data.\n",
    "    - __2. Regularization:__ (C parameter in python’s sklearn library) \n",
    "    Tells the SVM optimization how much you want to avoid misclassifying each training example.\n",
    "        - a. __Large values of C__: the optimization will choose a __smaller-margin__ hyperplane if that hyperplane does a better job of getting all the training points classified correctly.\n",
    "        - b. __very small value of C__: will cause the optimizer to look for a __larger-margin__ separating hyperplane, even if that hyperplane misclassifies more points.\n",
    "        <img src=\"Image/reg.png\">\n",
    "    - __3. Gamma:__  \n",
    "    Defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’:\n",
    "        - __Low gamma:__ points far away from plausible seperation line are considered in calculation for the seperation line.\n",
    "        - __High gamma:__ the points close to plausible line are considered in calculation.\n",
    "        <img src=\"Image/gamma.png\">\n",
    "    - __4. Margin:__  \n",
    "        - A separation of line to the closest class points.\n",
    "        - Very importrant characteristic of SVM classifier. SVM to core tries to achieve a good margin.\n",
    "        - __Good margin:__ one where this separation is larger for both the classes. (allows the points to be in their respective classes without crossing to other class.)\n",
    "        <img src=\"Image/margin2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bit about Bayes:\n",
    "#### 1. Bayes Theorem:\n",
    "  <img src=\"Image/b.png\">\n",
    "- e.g.: the probability that price of a house is high, can be better assessed if we know the facilities around it, compared to the assessment made without the knowledge of location of the house.   \n",
    "\n",
    "#### 2. What it for?\n",
    "- its possible to build a learner that predicts the probability of the response variable belonging to some class, given a new set of attributes.  \n",
    "\n",
    "#### 3. Naive Bayes Algorithm:  \n",
    "- __What?__ \n",
    "    - Not a single algorithm but a family of algorithms;\n",
    "    - Making an assumption of conditional independence over the training dataset. This drastically reduces the complexity of above mentioned problem to just 2n. (This assumption makes the Bayes algorithm, naive.)  \n",
    "- __Where?__\n",
    "    - __Text classification/ Spam Filtering/ Sentiment Analysis:__    \n",
    "        - Due to better result in multi class problems and independence rule  \n",
    "        - Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)  \n",
    "        - Recommendation System: filter unseen information and predict whether a user would like a given resource or not.  \n",
    "    - __Real time Prediction:__       \n",
    "        - Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.  \n",
    "    - __Multi class Prediction:__   \n",
    "        - predict the probability of multiple classes of target variable.  \n",
    "        \n",
    "- __How?__    \n",
    "    Given, n different attribute values, the likelihood now can be written as:  \n",
    "(X： the attributes or features  \n",
    " Y： variable.  \n",
    " P(X|Y) becomes equal to the products of, probability distribution of each attribute X given Y.)  \n",
    "<img src=\"Image/b2.png\">\n",
    "- __Why?__    \n",
    "    - Work really well in __complex situations__, despite the simplified assumptions and naivety.  \n",
    "    - Require __small number of training data__ for estimating the parameters necessary for classification.<br>\n",
    "\n",
    "- __Type:__ \n",
    "    - __Gaussian__: \n",
    "        - It is used in classification  \n",
    "        - Assumes that features follow a <font color='red'>__normal distribution.__</font>\n",
    "    - __Bernoulli__:  \n",
    "        - The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). \n",
    "        - One application would be text classification with ‘bag of words’ model where the __1s & 0s are “word occurs in the document”__ and __“word does not occur in the document” respectively__.  \n",
    "    - __Multinomial__:   \n",
    "        - It is used for discrete counts.  \n",
    "        - e.g.: text classification problem: Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have __“count how often word occurs in the document”__, or “number of times outcome number x_i is observed over the n trials”.  \n",
    "\n",
    "- __Examples:__\n",
    "    - A fruit may be considered to be an apple if it is red, round, and about 3″ in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features.  \n",
    "    - Predict the class of another fruit based on training set:\n",
    "        - From 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow\n",
    "        - Out of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow\n",
    "        - From the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow\n",
    "        <img src = 'Image/fruit.png'>\n",
    "        - Conclusion: In this case, based on the higher score (0.01875 lt 0.252) we can assume this Long, Sweet and Yellow fruit is, in fact, a __Banana__.  \n",
    "     \n",
    "- __Disadvantage:__\n",
    "    - Main disadvantage is that it <font color='red'> can’t learn interactions between features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bayes Network:\n",
    "- A type of probabilistic graphical model that uses Bayesian inference for __probability computations__. (directed acyclic graph)\n",
    "- __For what?__\n",
    "    - Aim to model __conditional dependence__, and therefore __causation__, by representing conditional dependence by edges in a directed graph. \n",
    "    <img src='Image/b3.jpeg'>\n",
    "- __Meaning:__ \n",
    "    - __edge (A, B)__: it means that P(B|A) is a factor in the joint probability distribution, so we must know P(B|A) for all values of B and A in order to conduct inference. \n",
    "    - e.g.: P(WetGrass|Rain) will be a factor, whose probability values are specified next to the WetGrass node in a conditional probability table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Bias:\n",
    "- In most cases, you can just treat the bias just like any other weight (so it might get initialised to some small random value), and it will get updated as you train your network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Disriminant Analysis:\n",
    "- __Why?__\n",
    "    - To develop discriminant functions that are nothing but the __linear combination of independent variables__ that will discriminate between the categories of the dependent variable in a perfect manner.\n",
    "    - It enables the researcher to examine whether significant differences exist among the groups, in terms of the predictor variables. \n",
    "    - It also evaluates the accuracy of the classification.\n",
    "- e.g.:  \n",
    "    - it can be used to understand the characteristics or the attributes of a customer possessing store loyalty and a customer who does not have store loyalty.\n",
    "    - It can be used to know whether heavy, medium and light users of soft drinks are different in terms of their consumption of frozen foods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Heterogeneous Ensemble: Bagging, boosting, stacking:\n",
    "- __Why?__\n",
    "    - Bagging to decrease the model’s variance, reduce overfitting by resampling data from the training set with the same cardinality as the original set.\n",
    "    - Boosting to decreasing the model’s bias;  \n",
    "    - Stacking to increasing the predictive force of the classifier.\n",
    "    <img src='Image/bias.png'>\n",
    "- __Why? Ensemble method?__\n",
    "    - To increase the stability of the final model and reduce the errors mentioned previously. \n",
    "    - By combining many models, we’re able to (mostly) _reduce the variance_, even when they are individually not great, as we won’t suffer from random errors from a single source.  \n",
    "- __Bagging:__\n",
    "    - __What?__\n",
    "        - The simplest approach with replacement is to use a couple of small subsamples and bag them,\n",
    "        - If the ensemble accuracy is much higher than the base models, it’s working; if not, use larger subsamples. (Note that using larger subsamples is not guaranteed to improve your results. )\n",
    "        <img src='Image/bagging.jpeg'>  \n",
    "    - __When?__\n",
    "        - limited data, and by using samples you’re able to get an estimate by aggregating the scores over many samples.\n",
    "    - __How? In code__\n",
    "    <img src='Image/bagcode.png'>\n",
    "    - __Voting:__\n",
    "        - 2 types of voting you can do for classifiers: hard and soft.\n",
    "        - Hard voting: need a majority of classifiers to determine what the result could be. like:\n",
    "        <img src=\"Image/baggingNN.png\">\n",
    "        <img src='Image/hot.png'>\n",
    "            * With our bagged ensemble results, we have an increase in accuracy(.658 against .641) and a decrease in variance (.091 against .094), so our ensemble model is working as expected after we’ve combined all the various models into one.\n",
    "        - Soft voting: compute a percentage weight with each classifier, multiply and finally averaged. (In reality weights are hard to find if you’re just providing your best guesses to which model you think should be weighted more or less--a linear optimization equation or __neural net__ could be constructed to find the correct weighting for each of the models to optimize the accuracy of the ensemble.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Boosting:__\n",
    "    - __What?__\n",
    "        - The main idea of boosting is to add additional models to the overall ensemble model sequentially. (The algorithm __creates multiple weak models__ whose output is added together to get an overall prediction.)\n",
    "        - This time with each iteration of boosting, a new model is created and the new base-learner model is trained (updated) from the errors of the previous learners.\n",
    "        - __Advantage:__ Maximize benefits; and Minimize downsides:\n",
    "    <img src='Image/boosting.png'>\n",
    "    <img src='Image/boosting2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Stacking:__\n",
    "    - A new model is trained from the combined predictions of two (or more) previous model. \n",
    "    - Ensemble stacking can be referred to as blending, because all the numbers are blended to produce a prediction or classification.\n",
    "    - __Why?__  \n",
    "        The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. \n",
    "    - Note:  just by adding layers and more models to your stacking algorithm, does not mean you’ll get a better predictor.\n",
    "    - __How?__  \n",
    "        __Step1:__ Build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model.  \n",
    "        __Step2:__ Add __a new model which learns from the intermediate predictions__ the same target.   \n",
    "        __Step3:__ This final model is said to be stacked on the top of the others.  \n",
    "        <img src='Image/stack2.png'>\n",
    "        \n",
    "    - e.g.: taken the vanilla classifiers [‘Random Forest’, ‘Extra Trees’, ‘KNeighbors’, ‘SVC’, ‘Ridge Classifier’].  \n",
    "    We started with, and combined them into all possible combinations to test which will perform best in our stacked model.\n",
    "        like: __[‘SVC’, ‘Ridge Classifier’], [‘SVC’], [‘Random Forest’, ‘Extra Trees’, ‘KNeighbors’], etc.__  \n",
    "        -- Once this is complete, we should have something to compare to our previous two results.  \n",
    "        Using a __logistic regression__ as the final output layer, as we still want to classify out housing data.\n",
    "        <img src='Image/stack.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Gradient Descent:\n",
    "   - Give the cost function; Iteratively; \n",
    "    <img src='Image/GD.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
